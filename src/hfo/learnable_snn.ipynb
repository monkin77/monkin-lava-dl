{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e62b37e-35dc-45b2-a025-31cf9ee971c5",
   "metadata": {},
   "source": [
    "# SNN to detect HFOs in iEEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "afc7d708-0431-4b60-91f0-9b30edbedac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cfd9",
   "metadata": {},
   "source": [
    "# Adjust the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8c804465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis/lava-dl/src/hfo\n"
     ]
    }
   ],
   "source": [
    "# Print current Working Directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"lava-dl/src/hfo\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/lava-dl/src/hfo/\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f76df0",
   "metadata": {},
   "source": [
    "# Check if GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "820134f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff3649-9c82-4fd4-bdbe-e4672f726dc7",
   "metadata": {},
   "source": [
    "# Create Network\n",
    "\n",
    "A slayer network definition follows standard PyTorch way using `torch.nn.Module`.\n",
    "\n",
    "The network can be described with a combination of individual `synapse`, `dendrite`, `neuron` and `axon` components. For rapid and easy development, slayer provides __block interface__ - `slayer.block` - which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n",
    "\n",
    "In the example below, `slayer.block.cuba` is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4172d38f-7d39-475f-bac8-7985fb1baa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # TODO: Change the parameters of the neuron\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "        }\n",
    "        # Add dropout to the network\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                # Input of 2 neurons (spike train UP and DOWN) and output of 256 neurons\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 2, 256,\n",
    "                                        weight_norm=True,\n",
    "                                        delay=True\n",
    "                                    ),\n",
    "                \n",
    "                # Final Layer with 2 neurons that spike if a Ripple or Fast Ripple are detected respectively\n",
    "                slayer.block.cuba.Dense(neuron_params, 256, 2, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617ab25-f112-42c6-8bdd-28d0ded7ffb1",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, DataSet and DataLoader\n",
    "\n",
    "Running the network in GPU is as simple as selecting `torch.device('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "47d40cfa-7c30-4192-910c-1b5a90e08c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "# net.double()\n",
    "# net.float()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c193c2c",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d239",
   "metadata": {},
   "source": [
    "### Import the Dataset to a numpy array\n",
    "\n",
    "Since the `lava-dl` package has a symbolic link to the `lava` package, we can use the `utils` module from the `lava` package directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "35b659ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: (245760, 2).\n",
      "Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example of how to read input spikes\n",
    "from utils.io import preview_np_array\n",
    "\n",
    "input_filename = 'snn_input_ripple_5_-5.npy'\n",
    "input_spikes = np.load(f\"data/{input_filename}\")\n",
    "\n",
    "preview_np_array(input_spikes, \"Input Spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed654d",
   "metadata": {},
   "source": [
    "### Define the `Datasets` and `DataLoaders`\n",
    "\n",
    "`PyTorch` provides 2 data primitives - `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` to work with data.\n",
    "- `Dataset` stores the samples and their corresponding labels.\n",
    "- `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "855431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: torch.Size([245760, 2]).\n",
      "Preview: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        ...,\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "Annotations Shape: torch.Size([245760]).\n",
      "Preview: tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.ripple_spike_trains_dataset import SpikeTrainsDataset\n",
    "\n",
    "# Load the Dataset by providing the filename\n",
    "input_filename = 'data/snn_input_ripple_5_-5.npy'\n",
    "annotations_filename = 'ground_truth/ch-63.npy'\n",
    "snn_dataset = SpikeTrainsDataset(input_filename, annotations_filename, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e869a",
   "metadata": {},
   "source": [
    "### Split the data into training, validation and test sets\n",
    "\n",
    "Need to evaluate the need for a validation set. For now, we will use a 70/10/20 split for training, validation and test sets respectively.\n",
    "\n",
    "The **spike trains data is ordered**, so we **cannot shuffle** the data as it would disrupt the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "efa1e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into Training and Validation+Testing tests\n",
    "# 70% Training, 30% Validation+Testing\n",
    "# Since we are working with temporal data, we will split the dataset in order (NO SHUFFLE)\n",
    "train_dataset, val_test_dataset = train_test_split(snn_dataset, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the Validation+Testing dataset into Validation and Testing sets\n",
    "# 10% Validation, 20% Testing\n",
    "val_dataset, test_dataset = train_test_split(val_test_dataset, test_size=0.67, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f324e",
   "metadata": {},
   "source": [
    "### Create the `DataLoader` objects\n",
    "The `batch_size` parameter determines the number of samples that will be loaded and passed through the network at once during training. It's a form of stochastic gradient where instead of updating the weights after each sample, you can update the weights based on a subset of data (a batch).\n",
    "\n",
    "When working with temporal sequential data, the order is very important. However, the `batch_size` does not directly affect the order of the data. In fact, when working with time series data, it might be relevant to use sequences/windows of data as input for the model. For example, if we want to predict the next value of a sequence, we can use a window of `n` samples as input. In this case, each \"sample\" in your batch would actually be a sequence of `n` values.\n",
    "\n",
    "Once again, shuffle is set to `False` to maintain the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1f648",
   "metadata": {},
   "source": [
    "### **Batch Size**\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In our case, it makes sense to use a batch size equal to the ***prediction window*** size. This way, we can identify the presence of Ripple or Fast Ripples using the entire window of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ac1113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch_size / window_size\n",
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e5463bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for the Training, Validation and Testing Datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=window_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=window_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=window_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1551e-ce69-46ef-b1bc-d73be3c97794",
   "metadata": {},
   "source": [
    "# Visualize the input data\n",
    "\n",
    "A `slayer.io.Event` can be visualized by invoking it's `Event.show()` routine. `Event.anim()` instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cafc3a",
   "metadata": {},
   "source": [
    "To visualize the input data as a gif, we need to convert each time step into an image. We can use the `Event.show()` method to do this. The `Event.show()` method returns a `matplotlib` figure which can be saved as an image. We can then use the `imageio` package to convert the images into a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5a6dadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each time step into an image\n",
    "img_height = 25\n",
    "img_width = 25\n",
    "\n",
    "spike_width = 3     # Width of the spike\n",
    "spike_interval = 2  # Space between spikes\n",
    "spike_positions = [0, 5, 10, 15, 20]    # Positions of the spikes based on spike_width and spike_interval\n",
    "\n",
    "y_padding = 5\n",
    "up_spike_y = img_height - y_padding\n",
    "down_spike_y = y_padding\n",
    "\n",
    "def color_pixels(batch, img, idx, x_position_idx):\n",
    "    # Check if batch[idx] had an UP spike\n",
    "    if batch[idx][0] == 1:\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[up_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1\n",
    "    \n",
    "    # Check if batch[idx] had a DOWN spike\n",
    "    if batch[idx][1] == 1:\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[down_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3342afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_image_array(batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of spikes into an array of images\n",
    "    \"\"\"\n",
    "    # Create new batch object with the shape (window_size, img_height, img_width)\n",
    "    new_batch = np.ndarray(shape=(len(batch), img_height, img_width))\n",
    "\n",
    "    for idx in range(len(batch)):\n",
    "        # Initialize the image as a 2D array of zeros\n",
    "        img = np.zeros(shape=(img_height, img_width))\n",
    "\n",
    "        # Color the middle line to represent the current time step (Width 1)\n",
    "        img[1:, img_width//2] = 0.5\n",
    "\n",
    "        # --- Color the pixels where a spike is detected as 1 --- # \n",
    "        if idx >= 2:\n",
    "            # Color the pixels of the t-2 time step\n",
    "            color_pixels(batch, img, idx-2, 0)\n",
    "        if idx >= 1:\n",
    "            # Color the pixels of the t-1 time step\n",
    "            color_pixels(batch, img, idx-1, 1)\n",
    "        # Color the pixels of the current time step\n",
    "\n",
    "        color_pixels(batch, img, idx, 2)\n",
    "        if idx+1 < len(batch):\n",
    "            # Color the pixels of the t+1 time step\n",
    "            color_pixels(batch, img, idx+1, 3)\n",
    "        if idx+2 < len(batch):\n",
    "            # Color the pixels of the t+2 time step\n",
    "            color_pixels(batch, img, idx+2, 4)\n",
    "        \n",
    "        # Update the new_batch with the current batch corresponding image\n",
    "        new_batch[idx] = img\n",
    "        \n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "be0b1c3b-77ec-4d8d-9fb3-22ae2b6dd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import make_gif\n",
    "\n",
    "skip_iter = 80\n",
    "max_iter = 10\n",
    "\n",
    "EXPORT_GIFS = False\n",
    "if EXPORT_GIFS:\n",
    "    for (i, batch) in enumerate(train_loader):\n",
    "        if i < skip_iter:\n",
    "            continue\n",
    "\n",
    "        if i - skip_iter >= max_iter:\n",
    "            break\n",
    "\n",
    "        # Fetch an input and target from the training dataset\n",
    "        train_features, train_label = batch\n",
    "\n",
    "        # print(\"\\nIdx:\", i)\n",
    "        \n",
    "        # Convert the batch of spikes into an array of images\n",
    "        batch_img_array = batch_to_image_array(train_features)\n",
    "        # preview_np_array(batch_img_array, \"Image Array\")\n",
    "        \n",
    "        # Reshape the batch_img_array, adding 5 frames at the start full of zeros to represent the beggining of the gif\n",
    "        batch_img_array = np.concatenate((np.zeros((5, img_height, img_width)), batch_img_array), axis=0)\n",
    "\n",
    "        # Create a GIF from the images using Pillow\n",
    "        make_gif(batch_img_array, f\"data/gifs/input-{i}.gif\", duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397244cb",
   "metadata": {},
   "source": [
    "### Display Some of the GIFs representing input examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6429",
   "metadata": {},
   "source": [
    "Below you can see batches 84 and 85 respectively.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"./data/gifs/input-84.gif\" width=\"200\"/> </td>\n",
    "        <td style=\"border:0;\"> </td>\n",
    "        <td style=\"border:0;\"> </td>\n",
    "        <td> <img src=\"./data/gifs/input-85.gif\" width=\"200\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b3eb2",
   "metadata": {},
   "source": [
    "# Convert the Input to a Tensor shape supported by the Slayer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cec3e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_data(batch):\n",
    "    \"\"\"\n",
    "    Format the batch data into the correct format for the network\n",
    "    \"\"\"\n",
    "    # print(f\"batch length: {len(batch)}\")\n",
    "\n",
    "    # Convert the batch into a tensor\n",
    "    batch = torch.tensor(batch, dtype=torch.float64)\n",
    "\n",
    "    # -- Create tensor with 4/5 dimensions (Batch (optional), Channels, Height, Width, Time) --\n",
    "    # The batch and Channels dimensions already exist in the object\n",
    "    # Add the Height and Width dimensions\n",
    "    batch = batch.unsqueeze(2)\n",
    "    batch = batch.unsqueeze(3)\n",
    "    # Add the Time dimension\n",
    "    batch = batch.unsqueeze(4)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "71e5ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Data Shape: torch.Size([100, 2, 1, 1, 1]).\n",
      "Preview: tensor([[[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]],\n",
      "\n",
      "\n",
      "         [[[0.]]]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118841/3801305566.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = torch.tensor(batch, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Testing the format_batch_data function\n",
    "train_batch, _ = next(iter(train_loader))\n",
    "formatted_data = format_batch_data(train_batch)\n",
    "# preview_np_array(formatted_data, \"Formatted Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3c0121dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Input Shape: torch.Size([2, 1, 1, 1]).\n",
      "Preview: tensor([[[[0.]]],\n",
      "\n",
      "\n",
      "        [[[0.]]]], dtype=torch.float64)\n",
      "Formatted Target Shape: torch.Size([1, 1, 1, 1]).\n",
      "Preview: tensor([[[[0.]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input to a tensor shape supported by Slayer\n",
    "# It is assumed that all the tensors that are being processed are 5 dimensional. \n",
    "# (Batch, Channels, Height, Width, Time) or NCHWT format. \n",
    "# The user must make sure that an input of correct dimension is supplied.\n",
    "# TODO: I guess the time must be fed to the model. IT can be calculated in hte SpikeTrainsDataset as long as it knows the sampling rate.\n",
    "input, target = train_dataset[0]\n",
    "# preview_np_array(input, \"Input\")\n",
    "\n",
    "# -- Create tensor with 4/5 dimensions (Batch (optional), Channels, Height, Width, Time) --\n",
    "# The channels dimension is already included in the original input (2 channels)\n",
    "# Add the height and width dimensions\n",
    "formatted_input = torch.unsqueeze(input, 1)\n",
    "formatted_input = torch.unsqueeze(formatted_input, 2)\n",
    "# Add the time dimension\n",
    "formatted_input = torch.unsqueeze(formatted_input, 3)\n",
    "preview_np_array(formatted_input, \"Formatted Input\")\n",
    "\n",
    "# Reformat the target tensor as well\n",
    "# Add the channel dimension\n",
    "formatted_target = torch.unsqueeze(target, 0)\n",
    "# Add the height and width dimensions\n",
    "formatted_target = torch.unsqueeze(formatted_target, 1)\n",
    "formatted_target = torch.unsqueeze(formatted_target, 2)\n",
    "# Add the time dimension\n",
    "formatted_target = torch.unsqueeze(formatted_target, 3)\n",
    "preview_np_array(formatted_target, \"Formatted Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3fc61-6560-40fa-a222-5c051dc2ede7",
   "metadata": {},
   "source": [
    "# Error module\n",
    "\n",
    "Slayer provides prebuilt loss modules: `slayer.loss.{SpikeTime, SpikeRate, SpikeMax}`.\n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b121d44",
   "metadata": {},
   "source": [
    "Since the target spike train $\\hat{\\boldsymbol s}(t)$ is known for this problem, we use `SpikeTime` loss here. It uses _van Rossum_ like spike train distance metric. The actual and target spike trains are filtered using a FIR filter and the norm of the timeseries is the loss metric.\n",
    "\n",
    "$$L = \\frac{1}{2T} \\int_T \\left(h_\\text{FIR} * ({\\boldsymbol s} - \\hat{\\boldsymbol s})\\right)(t)^\\top{\\bf 1}\\,\\text dt $$\n",
    "\n",
    "* `time_constant`: time constant of the FIR filter.\n",
    "* `filter_order`: the order of FIR filter. Exponential decay is first order filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3c4b9b92-925a-4da8-b146-923dc4b6ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# TODO: Check values of the arguments\n",
    "# TODO: Could be another loss function? SpikeMoid?\n",
    "\n",
    "error = slayer.loss.SpikeMax(\n",
    "    moving_window=window_size,\n",
    "    # time_constant=2, \n",
    "    # filter_order=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd03d1",
   "metadata": {},
   "source": [
    "### Spike Time Loss Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ce524585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Input Shape: torch.Size([2, 1, 1, 1]).\n",
      "Preview: tensor([[[[0.]]],\n",
      "\n",
      "\n",
      "        [[[0.]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "net_input = formatted_input.to(device)\n",
    "preview_np_array(net_input, \"Net Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2ef46caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m net(net_input)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# just considering first neuron for illustration\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output_trace \u001b[38;5;241m=\u001b[39m \u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_target\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m target_trace \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mforward(target\u001b[38;5;241m.\u001b[39mto(device), output)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     12\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/loss.py:204\u001b[0m, in \u001b[0;36mSpikeMax.forward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# transpose the time dimension to the end\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# (batch, time, num_class) -> (batch, num_class, time)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    203\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mconfidence(\u001b[38;5;28minput\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogsoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/classifier.py:216\u001b[0m, in \u001b[0;36mMovingWindow.confidence\u001b[0;34m(self, spike, mode)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfidence\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Moving window confidence.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    >>> confidence = classifier.confidence(spike)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     sliding_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    217\u001b[0m         spike\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, spike\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/classifier.py:193\u001b[0m, in \u001b[0;36mMovingWindow.rate\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Moving window spike rate.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    >>> rate = classifier.rate(spike)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/filter/fir.py:54\u001b[0m, in \u001b[0;36mFIR.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/filter/conv.py:194\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(input, filter, sampling_time)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/filter/conv.py:118\u001b[0m, in \u001b[0;36m_conv.forward\u001b[0;34m(ctx, input, filter, sampling_time)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    109\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     ),\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/filter/conv.py:78\u001b[0m, in \u001b[0;36mfwd\u001b[0;34m(input, filter, sampling_time)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Accelerated\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mfwd(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m     83\u001b[0m         sampling_time\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/filter/conv.py:51\u001b[0m, in \u001b[0;36m_fwd\u001b[0;34m(input, filter, sampling_time)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fwd\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mfilter\u001b[39m, sampling_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m*\u001b[39m sampling_time\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "# The followng portion just illustrates the SpikeTime loss calculation. \n",
    "# IT IS NOT NEEDED IN PRACTICE\n",
    "\n",
    "# The network expects a 4D or 5D tensor as input (Batch (optional), Channels, Height, Width, Time)\n",
    "net_input = formatted_input.to(device)\n",
    "\n",
    "output = net(net_input)[0]\n",
    "\n",
    "# just considering first neuron for illustration\n",
    "output_trace = error.forward(output[0].to(device), formatted_target).flatten().cpu().data.numpy()\n",
    "target_trace = error.forward(target.to(device), output).flatten().cpu().data.numpy()\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 3), sharex=True)\n",
    "ax[0].plot(output_trace, label='output trace')\n",
    "ax[0].plot(target_trace, label='target trace')\n",
    "ax[1].plot(output_trace - target_trace, label='error trace')\n",
    "ax[0].set_ylabel('trace')\n",
    "ax[1].set_ylabel('trace')\n",
    "ax[1].set_xlabel('time [ms]')\n",
    "for a in ax: a.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1c953-78e7-47b7-9ce5-ab747ec23eb6",
   "metadata": {},
   "source": [
    "# Stats and Assistants\n",
    "\n",
    "Slayer provides `slayer.utils.LearningStats` as a simple learning statistics logger for training, validation and testing.\n",
    "\n",
    "In addtion, `slayer.utils.Assistant` module wraps common training validation and testing routine which help simplify the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "473884dd-d2fa-4e6a-b44d-6a1c303dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = slayer.utils.LearningStats()\n",
    "\n",
    "# Not specifying a Classifier. Which means we are using Regression mode (default)\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats,\n",
    "                                   classifier=slayer.classifier.MovingWindow(window_size)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb7069-d384-4368-8235-f5b782c5eeae",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Training loop mainly consists of looping over epochs and calling `assistant.train` utility to train.\n",
    "\n",
    "* `stats` can be used in print statement to get formatted stats printout.\n",
    "* `stats.training.best_loss` can be used to find out if the current iteration has the best loss. Here, we use it to save the best model.\n",
    "* `stats.testing.best_accuracy` can be used to find out if the current iteration has the best testing accuracy.\n",
    "* `stats.update()` updates the stats collected for the epoch.\n",
    "* `stats.save` saves the stats in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6a79047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118841/3801305566.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = torch.tensor(batch, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input tensor shape (torch.Size([100, 256, 1, 1, 1])) does not match with Neuron shape (torch.Size([1, 1])).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     formatted_input \u001b[38;5;241m=\u001b[39m format_batch_data(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/assistant.py:121\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         output, net_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[219], line 30\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 30\u001b[0m         spike \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/block/base.py:547\u001b[0m, in \u001b[0;36mAbstractDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m    546\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse(x)\n\u001b[0;32m--> 547\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_shift \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m     x \u001b[38;5;241m=\u001b[39m step_delay(\u001b[38;5;28mself\u001b[39m, x)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/neuron/cuba.py:439\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the full response of the neuron instance to an input.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    The input shape must match with the neuron shape. For the first time,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m    the neuron shape is determined from the input automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     _, voltage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspike(voltage)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/neuron/cuba.py:323\u001b[0m, in \u001b[0;36mNeuron.dynamics\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput tensor shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuron shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    328\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    329\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input tensor shape (torch.Size([100, 256, 1, 1, 1])) does not match with Neuron shape (torch.Size([1, 1]))."
     ]
    }
   ],
   "source": [
    "epochs = 1   # Number of epochs\n",
    "\n",
    "# TODO: Check the training loop implementation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, target) in enumerate(train_loader): # training loop\n",
    "        formatted_input = format_batch_data(input)\n",
    "\n",
    "        output = assistant.train(formatted_input, target)\n",
    "        print(f\"Training batch {i}/{len(train_loader)}\", end=\"\\r\")\n",
    "        \n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    if stats.training.best_loss:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "        \n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854daacd-5ea4-46fd-befd-b0918b26bda3",
   "metadata": {},
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "Plotting the learning curves is as easy as calling `stats.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59928f-4da2-4055-8802-71216003bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7785921-b165-425a-a3c8-600b1b822378",
   "metadata": {},
   "source": [
    "# Export the best model\n",
    "\n",
    "Load the best model during training and export it as hdf5 network. It is supported by `lava.lib.dl.netx` to automatically load the network as a lava process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a9efa-da94-45b9-8598-c669e522a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt'))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "net.export_hdf5(trained_folder + '/network.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a89584-2b71-49d9-b9e5-582e657676bc",
   "metadata": {},
   "source": [
    "# Visualize the network output\n",
    "\n",
    "Here, we will use `slayer.io.tensor_to_event` method to convert the torch output spike tensor into `slayer.io.Event` object and visualize a few input and output event pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37918a-c45d-40a5-b727-e2fe610b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input.to(device))\n",
    "for i in range(5):\n",
    "    # Reshape the input to the shape (sign_event, height_img, width_img, num_time_bins) to separate each DVS event\n",
    "    inp_event = slayer.io.tensor_to_event(input[i].cpu().data.numpy().reshape(2, 34, 34, -1))\n",
    "\n",
    "    # Reshape the output to a list containing the prediction percentage of each class\n",
    "    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n",
    "\n",
    "    inp_anim = inp_event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n",
    "    out_anim = out_event.anim(plt.figure(figsize=(10, 5)), frame_rate=240)\n",
    "    inp_anim.save(f'gifs/inp{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ed73-cdc1-43b8-8045-57cc9c499fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<table>'\n",
    "html += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\n",
    "for i in range(5):\n",
    "    html += '<tr>'\n",
    "    html += gif_td(f'gifs/inp{i}.gif')\n",
    "    html += gif_td(f'gifs/out{i}.gif')\n",
    "    html += '</tr>'\n",
    "html += '</tr></table>'\n",
    "display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
