{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e62b37e-35dc-45b2-a025-31cf9ee971c5",
   "metadata": {},
   "source": [
    "# SNN to detect HFOs in iEEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc7d708-0431-4b60-91f0-9b30edbedac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cfd9",
   "metadata": {},
   "source": [
    "# Adjust the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c804465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis\n",
      "File Location:  /home/monkin/Desktop/feup/thesis/lava-dl/src/hfo/\n",
      "New Working Directory:  /home/monkin/Desktop/feup/thesis/lava-dl/src/hfo\n"
     ]
    }
   ],
   "source": [
    "# Print current Working Directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"lava-dl/src/hfo\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/lava-dl/src/hfo/\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f76df0",
   "metadata": {},
   "source": [
    "# Check if GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820134f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff3649-9c82-4fd4-bdbe-e4672f726dc7",
   "metadata": {},
   "source": [
    "# Create Network\n",
    "\n",
    "A slayer network definition follows standard PyTorch way using `torch.nn.Module`.\n",
    "\n",
    "The network can be described with a combination of individual `synapse`, `dendrite`, `neuron` and `axon` components. For rapid and easy development, slayer provides __block interface__ - `slayer.block` - which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n",
    "\n",
    "In the example below, `slayer.block.cuba` is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4172d38f-7d39-475f-bac8-7985fb1baa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # TODO: Change the parameters of the neuron\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "        }\n",
    "        # Add dropout to the network\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                # Input of 2 neurons (spike train UP and DOWN) and output of 512 neurons\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 2, 256, weight_norm=True, delay=True),\n",
    "                \n",
    "                # Final Layer with 2 neurons that spike if a Ripple or Fast Ripple are detected respectively\n",
    "                slayer.block.cuba.Dense(neuron_params, 512, 2, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617ab25-f112-42c6-8bdd-28d0ded7ffb1",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, DataSet and DataLoader\n",
    "\n",
    "Running the network in GPU is as simple as selecting `torch.device('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d40cfa-7c30-4192-910c-1b5a90e08c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c193c2c",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d239",
   "metadata": {},
   "source": [
    "### Import the Dataset to a numpy array\n",
    "\n",
    "Since the `lava-dl` package has a symbolic link to the `lava` package, we can use the `utils` module from the `lava` package directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b659ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: (245760, 2).\n",
      "Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from utils.io import preview_np_array\n",
    "\n",
    "input_filename = 'snn_input_ripple_5_-5.npy'\n",
    "input_spikes = np.load(f\"data/{input_filename}\")\n",
    "\n",
    "preview_np_array(input_spikes, \"Input Spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed654d",
   "metadata": {},
   "source": [
    "### Define the `Datasets` and `DataLoaders`\n",
    "\n",
    "`PyTorch` provides 2 data primitives - `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` to work with data.\n",
    "- `Dataset` stores the samples and their corresponding labels.\n",
    "- `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: (245760, 2).\n",
      "Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.ripple_spike_trains_dataset import SpikeTrainsDataset\n",
    "\n",
    "# Load the Dataset by providing the filename\n",
    "input_filename = 'data/snn_input_ripple_5_-5.npy'\n",
    "annotations_filename = 'data/snn_annotations_ripple_5_-5.npy'\n",
    "snn_dataset = SpikeTrainsDataset(input_filename, annotations_filename, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e869a",
   "metadata": {},
   "source": [
    "### Split the data into training, validation and test sets\n",
    "\n",
    "Need to evaluate the need for a validation set. For now, we will use a 70/10/20 split for training, validation and test sets respectively.\n",
    "\n",
    "The **spike trains data is ordered**, so we **cannot shuffle** the data as it would disrupt the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa1e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into Training and Validation+Testing tests\n",
    "# 70% Training, 30% Validation+Testing\n",
    "# Since we are working with temporal data, we will split the dataset in order (NO SHUFFLE)\n",
    "train_dataset, val_test_dataset = train_test_split(snn_dataset, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the Validation+Testing dataset into Validation and Testing sets\n",
    "# 10% Validation, 20% Testing\n",
    "val_dataset, test_dataset = train_test_split(val_test_dataset, test_size=0.67, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f324e",
   "metadata": {},
   "source": [
    "### Create the `DataLoader` objects\n",
    "The `batch_size` parameter determines the number of samples that will be loaded and passed through the network at once during training. It's a form of stochastic gradient where instead of updating the weights after each sample, you can update the weights based on a subset of data (a batch).\n",
    "\n",
    "When working with temporal sequential data, the order is very important. However, the `batch_size` does not directly affect the order of the data. In fact, when working with time series data, it might be relevant to use sequences/windows of data as input for the model. For example, if we want to predict the next value of a sequence, we can use a window of `n` samples as input. In this case, each \"sample\" in your batch would actually be a sequence of `n` values.\n",
    "\n",
    "Once again, shuffle is set to `False` to maintain the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1f648",
   "metadata": {},
   "source": [
    "### **Batch Size**\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In our case, it makes sense to use a batch size equal to the ***prediction window*** size. This way, we can identify the presence of Ripple or Fast Ripples using the entire window of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch_size / window_size\n",
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5463bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for the Training, Validation and Testing Datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=window_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=window_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=window_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1551e-ce69-46ef-b1bc-d73be3c97794",
   "metadata": {},
   "source": [
    "# Visualize the input data\n",
    "\n",
    "A `slayer.io.Event` can be visualized by invoking it's `Event.show()` routine. `Event.anim()` instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cafc3a",
   "metadata": {},
   "source": [
    "To visualize the input data as a gif, we need to convert each time step into an image. We can use the `Event.show()` method to do this. The `Event.show()` method returns a `matplotlib` figure which can be saved as an image. We can then use the `imageio` package to convert the images into a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6dadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each time step into an image\n",
    "img_height = 25\n",
    "img_width = 25\n",
    "\n",
    "spike_width = 3     # Width of the spike\n",
    "spike_interval = 2  # Space between spikes\n",
    "spike_positions = [0, 5, 10, 15, 20]    # Positions of the spikes based on spike_width and spike_interval\n",
    "\n",
    "y_padding = 5\n",
    "up_spike_y = img_height - y_padding\n",
    "down_spike_y = y_padding\n",
    "\n",
    "def color_pixels(batch, img, idx, x_position_idx):\n",
    "    # Check if batch[idx] had an UP spike\n",
    "    if batch[idx][0] == 1:\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[up_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1\n",
    "    \n",
    "    # Check if batch[idx] had a DOWN spike\n",
    "    if batch[idx][1] == 1:\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[down_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3342afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_image_array(batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of spikes into an array of images\n",
    "    \"\"\"\n",
    "    # Create new batch object with the shape (window_size, img_height, img_width)\n",
    "    new_batch = np.ndarray(shape=(len(batch), img_height, img_width))\n",
    "\n",
    "    for idx in range(len(batch)):\n",
    "        # Initialize the image as a 2D array of zeros\n",
    "        img = np.zeros(shape=(img_height, img_width))\n",
    "\n",
    "        # Color the middle line to represent the current time step (Width 1)\n",
    "        img[1:, img_width//2] = 0.5\n",
    "\n",
    "        # --- Color the pixels where a spike is detected as 1 --- # \n",
    "        if idx >= 2:\n",
    "            # Color the pixels of the t-2 time step\n",
    "            color_pixels(batch, img, idx-2, 0)\n",
    "        if idx >= 1:\n",
    "            # Color the pixels of the t-1 time step\n",
    "            color_pixels(batch, img, idx-1, 1)\n",
    "        # Color the pixels of the current time step\n",
    "\n",
    "        color_pixels(batch, img, idx, 2)\n",
    "        if idx+1 < len(batch):\n",
    "            # Color the pixels of the t+1 time step\n",
    "            color_pixels(batch, img, idx+1, 3)\n",
    "        if idx+2 < len(batch):\n",
    "            # Color the pixels of the t+2 time step\n",
    "            color_pixels(batch, img, idx+2, 4)\n",
    "        \n",
    "        # Update the new_batch with the current batch corresponding image\n",
    "        new_batch[idx] = img\n",
    "        \n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0b1c3b-77ec-4d8d-9fb3-22ae2b6dd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import make_gif\n",
    "\n",
    "skip_iter = 80\n",
    "max_iter = 10\n",
    "\n",
    "EXPORT_GIFS = True\n",
    "if EXPORT_GIFS:\n",
    "    for (i, batch) in enumerate(train_loader):\n",
    "        if i < skip_iter:\n",
    "            continue\n",
    "\n",
    "        if i - skip_iter >= max_iter:\n",
    "            break\n",
    "\n",
    "        # Fetch an input and target from the training dataset\n",
    "        train_features, train_label = batch\n",
    "\n",
    "        # print(\"\\nIdx:\", i)\n",
    "        \n",
    "        # Convert the batch of spikes into an array of images\n",
    "        batch_img_array = batch_to_image_array(train_features)\n",
    "        # preview_np_array(batch_img_array, \"Image Array\")\n",
    "        \n",
    "        # Reshape the batch_img_array, adding 5 frames at the start full of zeros to represent the beggining of the gif\n",
    "        batch_img_array = np.concatenate((np.zeros((5, img_height, img_width)), batch_img_array), axis=0)\n",
    "\n",
    "        # Create a GIF from the images using Pillow\n",
    "        make_gif(batch_img_array, f\"data/gifs/input-{i}.gif\", duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397244cb",
   "metadata": {},
   "source": [
    "### Display Some of the GIFs representing input examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6429",
   "metadata": {},
   "source": [
    "Below you can see batches 84 and 85 respectively.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"./data/gifs/input-84.gif\" width=\"200\"/> </td>\n",
    "        <td style=\"border:0;\"> </td>\n",
    "        <td style=\"border:0;\"> </td>\n",
    "        <td> <img src=\"./data/gifs/input-85.gif\" width=\"200\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3fc61-6560-40fa-a222-5c051dc2ede7",
   "metadata": {},
   "source": [
    "# Error module\n",
    "\n",
    "Slayer provides prebuilt loss modules: `slayer.loss.{SpikeTime, SpikeRate, SpikeMax}`.\n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b121d44",
   "metadata": {},
   "source": [
    "Since the target spike train $\\hat{\\boldsymbol s}(t)$ is known for this problem, we use `SpikeTime` loss here. It uses _van Rossum_ like spike train distance metric. The actual and target spike trains are filtered using a FIR filter and the norm of the timeseries is the loss metric.\n",
    "\n",
    "$$L = \\frac{1}{2T} \\int_T \\left(h_\\text{FIR} * ({\\boldsymbol s} - \\hat{\\boldsymbol s})\\right)(t)^\\top{\\bf 1}\\,\\text dt $$\n",
    "\n",
    "* `time_constant`: time constant of the FIR filter.\n",
    "* `filter_order`: the order of FIR filter. Exponential decay is first order filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c4b9b92-925a-4da8-b146-923dc4b6ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# TODO: Check values of the arguments\n",
    "error = slayer.loss.SpikeTime(time_constant=2, filter_order=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd03d1",
   "metadata": {},
   "source": [
    "### Spike Time Loss Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef46caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The followng portion just illustrates the SpikeTime loss calculation. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# IT IS NOT NEEDED IN PRACTICE\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# just considering first neuron for illustration\u001b[39;00m\n\u001b[1;32m      7\u001b[0m output_trace \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mfilter(output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 27\u001b[0m         spike \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/block/base.py:546\u001b[0m, in \u001b[0;36mAbstractDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[0;32m--> 546\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynapse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron(z)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_shift \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/synapse/layer.py:172\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(  \u001b[38;5;66;03m# bias does not need pre_hook_fx. Its disabled\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(old_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, old_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    168\u001b[0m         weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    170\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(old_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, old_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 2]"
     ]
    }
   ],
   "source": [
    "# The followng portion just illustrates the SpikeTime loss calculation. \n",
    "# IT IS NOT NEEDED IN PRACTICE\n",
    "input, target = train_dataset[0]\n",
    "\n",
    "output = net(input.unsqueeze(dim=0).to(device))[0]\n",
    "# just considering first neuron for illustration\n",
    "output_trace = error.filter(output[0].to(device)).flatten().cpu().data.numpy()\n",
    "target_trace = error.filter(target[0].to(device)).flatten().cpu().data.numpy()\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 3), sharex=True)\n",
    "ax[0].plot(output_trace, label='output trace')\n",
    "ax[0].plot(target_trace, label='target trace')\n",
    "ax[1].plot(output_trace - target_trace, label='error trace')\n",
    "ax[0].set_ylabel('trace')\n",
    "ax[1].set_ylabel('trace')\n",
    "ax[1].set_xlabel('time [ms]')\n",
    "for a in ax: a.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1c953-78e7-47b7-9ce5-ab747ec23eb6",
   "metadata": {},
   "source": [
    "# Stats and Assistants\n",
    "\n",
    "Slayer provides `slayer.utils.LearningStats` as a simple learning statistics logger for training, validation and testing.\n",
    "\n",
    "In addtion, `slayer.utils.Assistant` module wraps common training validation and testing routine which help simplify the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473884dd-d2fa-4e6a-b44d-6a1c303dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = slayer.utils.LearningStats()\n",
    "\n",
    "# Not specifying a Classifier. Which means we are using Regression mode (default)\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb7069-d384-4368-8235-f5b782c5eeae",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Training loop mainly consists of looping over epochs and calling `assistant.train` utility to train.\n",
    "\n",
    "* `stats` can be used in print statement to get formatted stats printout.\n",
    "* `stats.training.best_loss` can be used to find out if the current iteration has the best loss. Here, we use it to save the best model.\n",
    "* `stats.testing.best_accuracy` can be used to find out if the current iteration has the best testing accuracy.\n",
    "* `stats.update()` updates the stats collected for the epoch.\n",
    "* `stats.save` saves the stats in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a79047d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [100, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/assistant.py:121\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         output, net_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 27\u001b[0m         spike \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/block/base.py:546\u001b[0m, in \u001b[0;36mAbstractDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[0;32m--> 546\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynapse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron(z)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_shift \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/synapse/layer.py:172\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(  \u001b[38;5;66;03m# bias does not need pre_hook_fx. Its disabled\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(old_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, old_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    168\u001b[0m         weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    170\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(old_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, old_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [100, 2]"
     ]
    }
   ],
   "source": [
    "epochs = 1   # Number of epochs\n",
    "\n",
    "# TODO: Check the training loop implementation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, target) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, target)\n",
    "        print(f\"Training batch {i}/{len(train_loader)}\", end=\"\\r\")\n",
    "        \n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    if stats.training.best_loss:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "        \n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854daacd-5ea4-46fd-befd-b0918b26bda3",
   "metadata": {},
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "Plotting the learning curves is as easy as calling `stats.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59928f-4da2-4055-8802-71216003bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7785921-b165-425a-a3c8-600b1b822378",
   "metadata": {},
   "source": [
    "# Export the best model\n",
    "\n",
    "Load the best model during training and export it as hdf5 network. It is supported by `lava.lib.dl.netx` to automatically load the network as a lava process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a9efa-da94-45b9-8598-c669e522a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt'))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "net.export_hdf5(trained_folder + '/network.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a89584-2b71-49d9-b9e5-582e657676bc",
   "metadata": {},
   "source": [
    "# Visualize the network output\n",
    "\n",
    "Here, we will use `slayer.io.tensor_to_event` method to convert the torch output spike tensor into `slayer.io.Event` object and visualize a few input and output event pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37918a-c45d-40a5-b727-e2fe610b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input.to(device))\n",
    "for i in range(5):\n",
    "    # Reshape the input to the shape (sign_event, height_img, width_img, num_time_bins) to separate each DVS event\n",
    "    inp_event = slayer.io.tensor_to_event(input[i].cpu().data.numpy().reshape(2, 34, 34, -1))\n",
    "\n",
    "    # Reshape the output to a list containing the prediction percentage of each class\n",
    "    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n",
    "\n",
    "    inp_anim = inp_event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n",
    "    out_anim = out_event.anim(plt.figure(figsize=(10, 5)), frame_rate=240)\n",
    "    inp_anim.save(f'gifs/inp{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ed73-cdc1-43b8-8045-57cc9c499fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<table>'\n",
    "html += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\n",
    "for i in range(5):\n",
    "    html += '<tr>'\n",
    "    html += gif_td(f'gifs/inp{i}.gif')\n",
    "    html += gif_td(f'gifs/out{i}.gif')\n",
    "    html += '</tr>'\n",
    "html += '</tr></table>'\n",
    "display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
