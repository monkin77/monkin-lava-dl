{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e62b37e-35dc-45b2-a025-31cf9ee971c5",
   "metadata": {},
   "source": [
    "# SNN to detect HFOs in iEEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "afc7d708-0431-4b60-91f0-9b30edbedac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cfd9",
   "metadata": {},
   "source": [
    "# Adjust the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c804465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis/lava-dl/src/hfo\n"
     ]
    }
   ],
   "source": [
    "# Print current Working Directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"lava-dl/src/hfo\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/lava-dl/src/hfo/\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f76df0",
   "metadata": {},
   "source": [
    "# Check if GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "820134f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff3649-9c82-4fd4-bdbe-e4672f726dc7",
   "metadata": {},
   "source": [
    "# Create Network\n",
    "\n",
    "A slayer network definition follows standard PyTorch way using `torch.nn.Module`.\n",
    "\n",
    "The network can be described with a combination of individual `synapse`, `dendrite`, `neuron` and `axon` components. For rapid and easy development, slayer provides __block interface__ - `slayer.block` - which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n",
    "\n",
    "In the example below, `slayer.block.cuba` is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4172d38f-7d39-475f-bac8-7985fb1baa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # TODO: Change the parameters of the neuron\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "        }\n",
    "        # Add dropout to the network\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                # Input of 2 neurons (spike train UP and DOWN) and output of 512 neurons\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 2, 256, weight_norm=True, delay=True),\n",
    "                \n",
    "                # Final Layer with 2 neurons that spike if a Ripple or Fast Ripple are detected respectively\n",
    "                slayer.block.cuba.Dense(neuron_params, 512, 2, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617ab25-f112-42c6-8bdd-28d0ded7ffb1",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, DataSet and DataLoader\n",
    "\n",
    "Running the network in GPU is as simple as selecting `torch.device('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "47d40cfa-7c30-4192-910c-1b5a90e08c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c193c2c",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d239",
   "metadata": {},
   "source": [
    "### Import the Dataset to a numpy array\n",
    "\n",
    "Since the `lava-dl` package has a symbolic link to the `lava` package, we can use the `utils` module from the `lava` package directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "35b659ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: (245760, 2).\n",
      "Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from utils.io import preview_np_array\n",
    "\n",
    "input_filename = 'snn_input_ripple_5_-5.npy'\n",
    "input_spikes = np.load(f\"data/{input_filename}\")\n",
    "\n",
    "preview_np_array(input_spikes, \"Input Spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed654d",
   "metadata": {},
   "source": [
    "### Define the `Datasets` and `DataLoaders`\n",
    "\n",
    "`PyTorch` provides 2 data primitives - `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` to work with data.\n",
    "- `Dataset` stores the samples and their corresponding labels.\n",
    "- `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "855431cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spikes Shape: (245760, 2).\n",
      "Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.ripple_spike_trains_dataset import SpikeTrainsDataset\n",
    "\n",
    "# Load the Dataset by providing the filename\n",
    "input_filename = 'data/snn_input_ripple_5_-5.npy'\n",
    "annotations_filename = 'data/snn_annotations_ripple_5_-5.npy'\n",
    "snn_dataset = SpikeTrainsDataset(input_filename, annotations_filename, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e869a",
   "metadata": {},
   "source": [
    "### Split the data into training, validation and test sets\n",
    "\n",
    "Need to evaluate the need for a validation set. For now, we will use a 70/10/20 split for training, validation and test sets respectively.\n",
    "\n",
    "The **spike trains data is ordered**, so we **cannot shuffle** the data as it would disrupt the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "efa1e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into Training and Validation+Testing tests\n",
    "# 70% Training, 30% Validation+Testing\n",
    "# Since we are working with temporal data, we will split the dataset in order (NO SHUFFLE)\n",
    "train_dataset, val_test_dataset = train_test_split(snn_dataset, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the Validation+Testing dataset into Validation and Testing sets\n",
    "# 10% Validation, 20% Testing\n",
    "val_dataset, test_dataset = train_test_split(val_test_dataset, test_size=0.67, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f324e",
   "metadata": {},
   "source": [
    "### Create the `DataLoader` objects\n",
    "The `batch_size` parameter determines the number of samples that will be loaded and passed through the network at once during training. It's a form of stochastic gradient where instead of updating the weights after each sample, you can update the weights based on a subset of data (a batch).\n",
    "\n",
    "When working with temporal sequential data, the order is very important. However, the `batch_size` does not directly affect the order of the data. In fact, when working with time series data, it might be relevant to use sequences/windows of data as input for the model. For example, if we want to predict the next value of a sequence, we can use a window of `n` samples as input. In this case, each \"sample\" in your batch would actually be a sequence of `n` values.\n",
    "\n",
    "Once again, shuffle is set to `False` to maintain the temporal sequence of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1f648",
   "metadata": {},
   "source": [
    "### **Batch Size**\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In our case, it makes sense to use a batch size equal to the ***prediction window*** size. This way, we can identify the presence of Ripple or Fast Ripples using the entire window of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ac1113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch_size / window_size\n",
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e5463bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for the Training, Validation and Testing Datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=window_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=window_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=window_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1551e-ce69-46ef-b1bc-d73be3c97794",
   "metadata": {},
   "source": [
    "# Visualize the input data\n",
    "\n",
    "A `slayer.io.Event` can be visualized by invoking it's `Event.show()` routine. `Event.anim()` instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cafc3a",
   "metadata": {},
   "source": [
    "To visualize the input data as a gif, we need to convert each time step into an image. We can use the `Event.show()` method to do this. The `Event.show()` method returns a `matplotlib` figure which can be saved as an image. We can then use the `imageio` package to convert the images into a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5a6dadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each time step into an image\n",
    "img_height = 25\n",
    "img_width = 25\n",
    "\n",
    "spike_width = 3     # Width of the spike\n",
    "spike_interval = 2  # Space between spikes\n",
    "spike_positions = [0, 5, 10, 15, 20]    # Positions of the spikes based on spike_width and spike_interval\n",
    "\n",
    "y_padding = 5\n",
    "up_spike_y = img_height - y_padding\n",
    "down_spike_y = y_padding\n",
    "\n",
    "def color_pixels(batch, img, idx, x_position_idx, curr_frame):\n",
    "    # Check if batch[idx] had an UP spike\n",
    "    if batch[idx][0] == 1:\n",
    "        print(\"Coloring Spike... at idx: \", idx, \"on frame: \", curr_frame)\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[up_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1\n",
    "    \n",
    "    # Check if batch[idx] had a DOWN spike\n",
    "    if batch[idx][1] == 1:\n",
    "        print(\"Coloring Spike... at idx: \", idx, \"on frame: \", curr_frame)\n",
    "        # Color the pixels where the spike is detected\n",
    "        img[down_spike_y][spike_positions[x_position_idx]:spike_positions[x_position_idx]+spike_width] = 1\n",
    "\n",
    "def batch_to_image_array(batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of spikes into an array of images that can be visualized using slayer\n",
    "    \"\"\"\n",
    "    # Create new batch object with the shape (num_channels, img_height, img_width, window_size)\n",
    "    # num_channels is 1\n",
    "    new_batch = np.ndarray(shape=(1, img_height, img_width, len(batch)))\n",
    "\n",
    "    for idx in range(len(batch)):\n",
    "        # Initialize the image as a 2D array of zeros\n",
    "        img = np.zeros(shape=(img_height, img_width))\n",
    "\n",
    "        # Color the middle line to represent the current time step (Width 1)\n",
    "        img[:, img_width//2] = 1\n",
    "\n",
    "        # --- Color the pixels where a spike is detected as 1 --- # \n",
    "        if idx >= 2:\n",
    "            # Color the pixels of the t-2 time step\n",
    "            color_pixels(batch, img, idx-2, 0, curr_frame=idx)\n",
    "        if idx >= 1:\n",
    "            # Color the pixels of the t-1 time step\n",
    "            color_pixels(batch, img, idx-1, 1, curr_frame=idx)\n",
    "        # Color the pixels of the current time step\n",
    "\n",
    "        color_pixels(batch, img, idx, 2, curr_frame=idx)\n",
    "        if idx+1 < len(batch):\n",
    "            # Color the pixels of the t+1 time step\n",
    "            color_pixels(batch, img, idx+1, 3, curr_frame=idx)\n",
    "        if idx+2 < len(batch):\n",
    "            # Color the pixels of the t+2 time step\n",
    "            color_pixels(batch, img, idx+2, 4, curr_frame=idx)\n",
    "        \n",
    "        # Update the new_batch with the current batch corresponding image\n",
    "        new_batch[0, :, :, idx] = img\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f53a23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coloring Spike... at idx:  96 on frame:  94\n",
      "Coloring Spike... at idx:  96 on frame:  95\n",
      "Coloring Spike... at idx:  96 on frame:  96\n",
      "Coloring Spike... at idx:  96 on frame:  97\n",
      "Coloring Spike... at idx:  96 on frame:  98\n",
      "Image Shape: (1, 25, 25, 100).\n",
      "Preview: [[[[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "for (i, batch) in enumerate(train_loader):\n",
    "    if i < 84:\n",
    "        continue\n",
    "\n",
    "    train_features, train_label = batch\n",
    "\n",
    "    # Convert the batch into an array of images\n",
    "    img_batch = batch_to_image_array(train_features)\n",
    "\n",
    "    preview_np_array(img_batch, \"Image\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "be0b1c3b-77ec-4d8d-9fb3-22ae2b6dd742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Idx: 80\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 81\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 82\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 83\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 84\n",
      "Coloring Spike... at idx:  96 on frame:  94\n",
      "Coloring Spike... at idx:  96 on frame:  95\n",
      "Coloring Spike... at idx:  96 on frame:  96\n",
      "Coloring Spike... at idx:  96 on frame:  97\n",
      "Coloring Spike... at idx:  96 on frame:  98\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 85\n",
      "Coloring Spike... at idx:  5 on frame:  3\n",
      "Coloring Spike... at idx:  5 on frame:  4\n",
      "Coloring Spike... at idx:  5 on frame:  5\n",
      "Coloring Spike... at idx:  5 on frame:  6\n",
      "Coloring Spike... at idx:  5 on frame:  7\n",
      "Coloring Spike... at idx:  10 on frame:  8\n",
      "Coloring Spike... at idx:  10 on frame:  9\n",
      "Coloring Spike... at idx:  10 on frame:  10\n",
      "Coloring Spike... at idx:  10 on frame:  11\n",
      "Coloring Spike... at idx:  10 on frame:  12\n",
      "Coloring Spike... at idx:  16 on frame:  14\n",
      "Coloring Spike... at idx:  16 on frame:  15\n",
      "Coloring Spike... at idx:  16 on frame:  16\n",
      "Coloring Spike... at idx:  16 on frame:  17\n",
      "Coloring Spike... at idx:  16 on frame:  18\n",
      "Coloring Spike... at idx:  20 on frame:  18\n",
      "Coloring Spike... at idx:  20 on frame:  19\n",
      "Coloring Spike... at idx:  20 on frame:  20\n",
      "Coloring Spike... at idx:  20 on frame:  21\n",
      "Coloring Spike... at idx:  20 on frame:  22\n",
      "Coloring Spike... at idx:  30 on frame:  28\n",
      "Coloring Spike... at idx:  30 on frame:  29\n",
      "Coloring Spike... at idx:  30 on frame:  30\n",
      "Coloring Spike... at idx:  30 on frame:  31\n",
      "Coloring Spike... at idx:  30 on frame:  32\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 86\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 87\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 88\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 89\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n",
      "\n",
      "Idx: 90\n",
      "spike tensor shape: torch.Size([1, 25, 25, 100])\n"
     ]
    }
   ],
   "source": [
    "skip_iter = 80\n",
    "max_iter = 10\n",
    "\n",
    "for (i, batch) in enumerate(train_loader):\n",
    "    if i < skip_iter:\n",
    "        continue\n",
    "\n",
    "    if i - skip_iter > max_iter:\n",
    "        break\n",
    "\n",
    "    # Fetch an input and target from the training dataset\n",
    "    train_features, train_label = batch\n",
    "\n",
    "    print(\"\\nIdx:\", i)\n",
    "    \n",
    "    # Convert the batch of spikes into an array of images\n",
    "    batch_img_array = batch_to_image_array(train_features)\n",
    "    # preview_np_array(batch_img_array, \"Image Array\")\n",
    "    \n",
    "    # Convert the image array into a tensor\n",
    "    spike_tensor = torch.from_numpy(batch_img_array).float().to(device)\n",
    "\n",
    "    print(\"spike tensor shape:\", spike_tensor.shape)\n",
    "\n",
    "    # Create the event from the tensor\n",
    "    slayer_event = slayer.io.tensor_to_event(spike_tensor, sampling_time=1.0)\n",
    "\n",
    "    anim = slayer_event.anim(plt.figure(figsize=(10, 10)), frame_rate=24, pre_compute_frames=False)\n",
    "    anim.save(f'data/gifs/input{i}.gif', animation.PillowWriter(fps=2), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89122bc4-6441-4963-bb43-2313432f0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 250px;\"/> </td>'\n",
    "header = '<table><tr>'\n",
    "images = ' '.join([gif_td(f'gifs/input{i}.gif') for i in range(5)])\n",
    "footer = '</tr></table>'\n",
    "display.HTML(header + images + footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3fc61-6560-40fa-a222-5c051dc2ede7",
   "metadata": {},
   "source": [
    "# Error module\n",
    "\n",
    "Slayer provides prebuilt loss modules: `slayer.loss.{SpikeTime, SpikeRate, SpikeMax}`.\n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n",
    "\n",
    "Since the target spike train is not known for this problem, we use `SpikeRate` loss and target high spiking rate for true class and low spiking rate for false class.\n",
    "\n",
    "target rate: $\\hat{\\boldsymbol r} = r_\\text{true}\\,{\\bf 1}[\\text{label}] + r_\\text{false}\\,(1-{\\bf 1}[\\text{label}])$ where ${\\bf 1}[\\text{label}]$ is one-hot encoding of label. The loss is:\n",
    "\n",
    "$$L = \\frac{1}{2} \\left(\\frac{1}{T}\\int_T {\\boldsymbol s}(t)\\,\\text dt -  \\hat{\\boldsymbol r}\\right)^\\top {\\bf 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b9b92-925a-4da8-b146-923dc4b6ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = slayer.loss.SpikeRate(true_rate=0.2, false_rate=0.03, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1c953-78e7-47b7-9ce5-ab747ec23eb6",
   "metadata": {},
   "source": [
    "# Stats and Assistants\n",
    "\n",
    "Slayer provides `slayer.utils.LearningStats` as a simple learning statistics logger for training, validation and testing.\n",
    "\n",
    "In addtion, `slayer.utils.Assistant` module wraps common training validation and testing routine which help simplify the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473884dd-d2fa-4e6a-b44d-6a1c303dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb7069-d384-4368-8235-f5b782c5eeae",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Training loop mainly consists of looping over epochs and calling `assistant.train` and `assistant.test` utilities over training and testing dataset. The `assistant` utility takes care of statndard backpropagation procedure internally.\n",
    "\n",
    "* `stats` can be used in print statement to get formatted stats printout.\n",
    "* `stats.testing.best_accuracy` can be used to find out if the current iteration has the best testing accuracy. Here, we use it to save the best model.\n",
    "* `stats.update()` updates the stats collected for the epoch.\n",
    "* `stats.save` saves the stats in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbc25a-41bb-49f0-bdb7-26be901626c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, label) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, label)\n",
    "        print(f\"Training epoch {i}/{len(train_loader)}\", end=\"\\r\")\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    for i, (input, label) in enumerate(test_loader): # training loop\n",
    "        output = assistant.test(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    if epoch%20 == 19: # cleanup display\n",
    "        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n",
    "        stats_str = str(stats).replace(\"| \", \"\\n\")\n",
    "        print(f'[Epoch {epoch:2d}/{epochs}]\\n{stats_str}')\n",
    "    \n",
    "    if stats.testing.best_accuracy:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')\n",
    "    net.grad_flow(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854daacd-5ea4-46fd-befd-b0918b26bda3",
   "metadata": {},
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "Plotting the learning curves is as easy as calling `stats.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59928f-4da2-4055-8802-71216003bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7785921-b165-425a-a3c8-600b1b822378",
   "metadata": {},
   "source": [
    "# Export the best model\n",
    "\n",
    "Load the best model during training and export it as hdf5 network. It is supported by `lava.lib.dl.netx` to automatically load the network as a lava process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a9efa-da94-45b9-8598-c669e522a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt'))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "net.export_hdf5(trained_folder + '/network.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a89584-2b71-49d9-b9e5-582e657676bc",
   "metadata": {},
   "source": [
    "# Visualize the network output\n",
    "\n",
    "Here, we will use `slayer.io.tensor_to_event` method to convert the torch output spike tensor into `slayer.io.Event` object and visualize a few input and output event pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37918a-c45d-40a5-b727-e2fe610b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input.to(device))\n",
    "for i in range(5):\n",
    "    # Reshape the input to the shape (sign_event, height_img, width_img, num_time_bins) to separate each DVS event\n",
    "    inp_event = slayer.io.tensor_to_event(input[i].cpu().data.numpy().reshape(2, 34, 34, -1))\n",
    "\n",
    "    # Reshape the output to a list containing the prediction percentage of each class\n",
    "    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n",
    "\n",
    "    inp_anim = inp_event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n",
    "    out_anim = out_event.anim(plt.figure(figsize=(10, 5)), frame_rate=240)\n",
    "    inp_anim.save(f'gifs/inp{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ed73-cdc1-43b8-8045-57cc9c499fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<table>'\n",
    "html += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\n",
    "for i in range(5):\n",
    "    html += '<tr>'\n",
    "    html += gif_td(f'gifs/inp{i}.gif')\n",
    "    html += gif_td(f'gifs/out{i}.gif')\n",
    "    html += '</tr>'\n",
    "html += '</tr></table>'\n",
    "display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
