{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e62b37e-35dc-45b2-a025-31cf9ee971c5",
   "metadata": {},
   "source": [
    "# SNN to detect HFOs in iEEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc7d708-0431-4b60-91f0-9b30edbedac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398cfd9",
   "metadata": {},
   "source": [
    "# Adjust the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c804465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis/lava-dl/src/hfo\n"
     ]
    }
   ],
   "source": [
    "# Print current Working Directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"lava-dl/src/hfo\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/lava-dl/src/hfo/\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f76df0",
   "metadata": {},
   "source": [
    "# Check if GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820134f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f871017",
   "metadata": {},
   "source": [
    "## Create the Custom Input Layer\n",
    "\n",
    "### Define function to read the input data from the csv file and generate the corresponding spike events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_spike_events(file_path: str):\n",
    "    \"\"\"Reads the spike events from the input file and returns them as a numpy array\n",
    "\n",
    "    @file_path (str): name of the file containing the spike events\n",
    "    \"\"\"\n",
    "    spike_events = []\n",
    "\n",
    "    try:\n",
    "        # Read the spike events from the file\n",
    "        df = pd.read_csv(file_path, header=True)\n",
    "\n",
    "        # Detect errors\n",
    "        if df.empty:\n",
    "            raise Exception(\"The input file is empty\")\n",
    "\n",
    "        # Convert the scientific notation values to integers if any exist\n",
    "        df = df.applymap(lambda x: int(float(x)) if (isinstance(x, str) and 'e' in x) else x)\n",
    "\n",
    "        # Convert the dataframe to a numpy array\n",
    "        spike_events = df.to_numpy()\n",
    "        return spike_events[0]\n",
    "    except Exception as e:\n",
    "        print(\"Unable to read the input file: \", file_path, \" error:\", e)\n",
    "\n",
    "    return spike_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdc6da",
   "metadata": {},
   "source": [
    "Since the `lava-dl` package has a symbolic link to the `lava` package, we can use the `utils` module from the `lava` package directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7ccee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Train UP events:  (307, 2) [[ 4153.3203125     63.        ]\n",
      " [ 4155.76171875    63.        ]\n",
      " [ 4165.52734375    63.        ]\n",
      " [ 7249.0234375     63.        ]\n",
      " [ 7272.4609375     63.        ]\n",
      " [ 7273.92578125    63.        ]\n",
      " [ 7285.15625       63.        ]\n",
      " [ 7286.62109375    63.        ]\n",
      " [13386.23046875    63.        ]\n",
      " [13397.4609375     63.        ]]\n",
      "Spike Train DOWN events:  (309, 2) [[ 4148.92578125    63.        ]\n",
      " [ 4158.69140625    63.        ]\n",
      " [ 4160.64453125    63.        ]\n",
      " [ 7242.67578125    63.        ]\n",
      " [ 7266.6015625     63.        ]\n",
      " [ 7278.80859375    63.        ]\n",
      " [ 7280.2734375     63.        ]\n",
      " [ 7291.015625      63.        ]\n",
      " [13382.8125        63.        ]\n",
      " [13393.5546875     63.        ]]\n"
     ]
    }
   ],
   "source": [
    "from utils.input import read_spike_events\n",
    "\n",
    "# Call the function to read the spike events\n",
    "ripple_up_spike_train_file = \"./data/ripple_up_spike_train_5.csv\"\n",
    "ripple_down_spike_train_file = \"./data/ripple_down_spike_train_-5.csv\"\n",
    "ripple_spike_train_up = read_spike_events(ripple_up_spike_train_file)\n",
    "ripple_spike_train_down = read_spike_events(ripple_down_spike_train_file)\n",
    "\n",
    "print(\"Spike Train UP events: \", ripple_spike_train_up.shape, ripple_spike_train_up[:10])\n",
    "print(\"Spike Train DOWN events: \", ripple_spike_train_down.shape, ripple_spike_train_down[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670955ec-b45f-4ce5-a0aa-acce71a07370",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "\n",
    "The dataset class follows standard torch dataset definition. They are defined in `nmnist.py`. We will just import the dataset and augmentation routine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36ceee15-929a-43e4-a63e-57772019c1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nmnist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augment, NMNISTDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nmnist'"
     ]
    }
   ],
   "source": [
    "from nmnist import augment, NMNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff3649-9c82-4fd4-bdbe-e4672f726dc7",
   "metadata": {},
   "source": [
    "# Create Network\n",
    "\n",
    "A slayer network definition follows standard PyTorch way using `torch.nn.Module`.\n",
    "\n",
    "The network can be described with a combination of individual `synapse`, `dendrite`, `neuron` and `axon` components. For rapid and easy development, slayer provides __block interface__ - `slayer.block` - which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n",
    "\n",
    "In the example below, `slayer.block.cuba` is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172d38f-7d39-475f-bac8-7985fb1baa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 34*34*2, 512, weight_norm=True, delay=True),    # Input of 34*34*2 and output of 512\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 512, 512, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 512, 10, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617ab25-f112-42c6-8bdd-28d0ded7ffb1",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, DataSet and DataLoader\n",
    "\n",
    "Running the network in GPU is as simple as selecting `torch.device('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d40cfa-7c30-4192-910c-1b5a90e08c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMNIST dataset is freely available here:\n",
      "https://www.garrickorchard.com/datasets/n-mnist\n",
      "\n",
      "(c) Creative Commons:\n",
      "    Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.\n",
      "    \"Converting Static Image Datasets to Spiking Neuromorphic Datasets Using\n",
      "    Saccades\",\n",
      "    Frontiers in Neuroscience, vol.9, no.437, Oct. 2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "training_set = NMNISTDataset(train=True, transform=augment)\n",
    "testing_set  = NMNISTDataset(train=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1551e-ce69-46ef-b1bc-d73be3c97794",
   "metadata": {},
   "source": [
    "# Visualize the input data\n",
    "\n",
    "A `slayer.io.Event` can be visualized by invoking it's `Event.show()` routine. `Event.anim()` instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b1c3b-77ec-4d8d-9fb3-22ae2b6dd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    spike_tensor, label = testing_set[np.random.randint(len(testing_set))]\n",
    "\n",
    "    # reshape the 2D tensor to the shape (sign_event, height_img, width_img, num_time_bins) to separate the events of each timestamp\n",
    "    spike_tensor = spike_tensor.reshape(2, 34, 34, -1)  \n",
    "    \n",
    "    event = slayer.io.tensor_to_event(spike_tensor.cpu().data.numpy())\n",
    "    anim = event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n",
    "    anim.save(f'gifs/input{i}.gif', animation.PillowWriter(fps=24), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89122bc4-6441-4963-bb43-2313432f0530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td> <img src=\"gifs/input0.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td> <td> <img src=\"gifs/input1.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td> <td> <img src=\"gifs/input2.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td> <td> <img src=\"gifs/input3.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td> <td> <img src=\"gifs/input4.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gif_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 250px;\"/> </td>'\n",
    "header = '<table><tr>'\n",
    "images = ' '.join([gif_td(f'gifs/input{i}.gif') for i in range(5)])\n",
    "footer = '</tr></table>'\n",
    "display.HTML(header + images + footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3fc61-6560-40fa-a222-5c051dc2ede7",
   "metadata": {},
   "source": [
    "# Error module\n",
    "\n",
    "Slayer provides prebuilt loss modules: `slayer.loss.{SpikeTime, SpikeRate, SpikeMax}`.\n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n",
    "\n",
    "Since the target spike train is not known for this problem, we use `SpikeRate` loss and target high spiking rate for true class and low spiking rate for false class.\n",
    "\n",
    "target rate: $\\hat{\\boldsymbol r} = r_\\text{true}\\,{\\bf 1}[\\text{label}] + r_\\text{false}\\,(1-{\\bf 1}[\\text{label}])$ where ${\\bf 1}[\\text{label}]$ is one-hot encoding of label. The loss is:\n",
    "\n",
    "$$L = \\frac{1}{2} \\left(\\frac{1}{T}\\int_T {\\boldsymbol s}(t)\\,\\text dt -  \\hat{\\boldsymbol r}\\right)^\\top {\\bf 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b9b92-925a-4da8-b146-923dc4b6ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = slayer.loss.SpikeRate(true_rate=0.2, false_rate=0.03, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1c953-78e7-47b7-9ce5-ab747ec23eb6",
   "metadata": {},
   "source": [
    "# Stats and Assistants\n",
    "\n",
    "Slayer provides `slayer.utils.LearningStats` as a simple learning statistics logger for training, validation and testing.\n",
    "\n",
    "In addtion, `slayer.utils.Assistant` module wraps common training validation and testing routine which help simplify the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473884dd-d2fa-4e6a-b44d-6a1c303dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb7069-d384-4368-8235-f5b782c5eeae",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Training loop mainly consists of looping over epochs and calling `assistant.train` and `assistant.test` utilities over training and testing dataset. The `assistant` utility takes care of statndard backpropagation procedure internally.\n",
    "\n",
    "* `stats` can be used in print statement to get formatted stats printout.\n",
    "* `stats.testing.best_accuracy` can be used to find out if the current iteration has the best testing accuracy. Here, we use it to save the best model.\n",
    "* `stats.update()` updates the stats collected for the epoch.\n",
    "* `stats.save` saves the stats in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbc25a-41bb-49f0-bdb7-26be901626c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 301/1875\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/utils/assistant.py:139\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    137\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m net_loss\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 139\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/thesis-lava/.venv/lib/python3.10/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/neuron/dynamics/leaky_integrator.py:166\u001b[0m, in \u001b[0;36m_LIDynamics.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m output, decay \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[0;32m--> 166\u001b[0m grad_input, grad_decay \u001b[38;5;241m=\u001b[39m \u001b[43m_li_dynamics_bwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _LIDynamics\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m grad_output\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     _grad_input, _grad_decay \u001b[38;5;241m=\u001b[39m Accelerated\u001b[38;5;241m.\u001b[39mleaky_integrator\u001b[38;5;241m.\u001b[39mbwd(\n\u001b[1;32m    170\u001b[0m         grad_output, output, decay\n\u001b[1;32m    171\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/feup/thesis/lava-dl/src/lava/lib/dl/slayer/neuron/dynamics/leaky_integrator.py:250\u001b[0m, in \u001b[0;36m_li_dynamics_bwd\u001b[0;34m(grad_output, output, decay)\u001b[0m\n\u001b[1;32m    247\u001b[0m grad_decay \u001b[38;5;241m=\u001b[39m grad_input[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m*\u001b[39m output[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnumel(decay) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# shared parameters\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     grad_decay \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_decay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     grad_decay \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(grad_decay, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs = 100\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, label) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, label)\n",
    "        print(f\"Training epoch {i}/{len(train_loader)}\", end=\"\\r\")\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    for i, (input, label) in enumerate(test_loader): # training loop\n",
    "        output = assistant.test(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    if epoch%20 == 19: # cleanup display\n",
    "        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n",
    "        stats_str = str(stats).replace(\"| \", \"\\n\")\n",
    "        print(f'[Epoch {epoch:2d}/{epochs}]\\n{stats_str}')\n",
    "    \n",
    "    if stats.testing.best_accuracy:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')\n",
    "    net.grad_flow(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854daacd-5ea4-46fd-befd-b0918b26bda3",
   "metadata": {},
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "Plotting the learning curves is as easy as calling `stats.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59928f-4da2-4055-8802-71216003bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7785921-b165-425a-a3c8-600b1b822378",
   "metadata": {},
   "source": [
    "# Export the best model\n",
    "\n",
    "Load the best model during training and export it as hdf5 network. It is supported by `lava.lib.dl.netx` to automatically load the network as a lava process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a9efa-da94-45b9-8598-c669e522a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt'))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(trained_folder + '/network.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "net.export_hdf5(trained_folder + '/network.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a89584-2b71-49d9-b9e5-582e657676bc",
   "metadata": {},
   "source": [
    "# Visualize the network output\n",
    "\n",
    "Here, we will use `slayer.io.tensor_to_event` method to convert the torch output spike tensor into `slayer.io.Event` object and visualize a few input and output event pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37918a-c45d-40a5-b727-e2fe610b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input.to(device))\n",
    "for i in range(5):\n",
    "    # Reshape the input to the shape (sign_event, height_img, width_img, num_time_bins) to separate each DVS event\n",
    "    inp_event = slayer.io.tensor_to_event(input[i].cpu().data.numpy().reshape(2, 34, 34, -1))\n",
    "\n",
    "    # Reshape the output to a list containing the prediction percentage of each class\n",
    "    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n",
    "\n",
    "    inp_anim = inp_event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n",
    "    out_anim = out_event.anim(plt.figure(figsize=(10, 5)), frame_rate=240)\n",
    "    inp_anim.save(f'gifs/inp{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ed73-cdc1-43b8-8045-57cc9c499fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr><tr><td> <img src=\"gifs/inp0.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td><td> <img src=\"gifs/out0.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr><tr><td> <img src=\"gifs/inp1.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td><td> <img src=\"gifs/out1.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr><tr><td> <img src=\"gifs/inp2.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td><td> <img src=\"gifs/out2.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr><tr><td> <img src=\"gifs/inp3.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td><td> <img src=\"gifs/out3.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr><tr><td> <img src=\"gifs/inp4.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td><td> <img src=\"gifs/out4.gif\" alt=\"Drawing\" style=\"height: 250px;\"/> </td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = '<table>'\n",
    "html += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\n",
    "for i in range(5):\n",
    "    html += '<tr>'\n",
    "    html += gif_td(f'gifs/inp{i}.gif')\n",
    "    html += gif_td(f'gifs/out{i}.gif')\n",
    "    html += '</tr>'\n",
    "html += '</tr></table>'\n",
    "display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
